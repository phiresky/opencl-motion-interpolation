{
	"_info": "Auto-generated by pandoc-url2cite. Feel free to modify, keys will never be overwritten.",
	"urls": {
		"https://en.wikipedia.org/wiki/Three-two_pull_down": {
			"fetched": "2019-12-17T23:18:53.397Z",
			"bibtex": [
				"",
				"@misc{noauthor_three-two_2019,",
				"   title = {Three-two pull down},",
				"   copyright = {Creative Commons Attribution-ShareAlike License},",
				"   url = {https://en.wikipedia.org/w/index.php?title=Three-two_pull_down&oldid=899459736},",
				"   abstract = {Three-two pull down (3:2 pull down) is a term used in filmmaking and television production for the post-production process of transferring film to video.",
				"It converts 24 frames per second into 29.97 frames per second. Roughly speaking, converting every 4 frames into 5 frames plus a slight slow down in speed. Film runs at a standard rate of 24 frames per second, whereas NTSC video has a signal frame rate of 29.97 frames per second. Every interlaced video frame has two fields for each frame. The three-two pull down is where the telecine adds a third video field (a half frame) to every second video frame, but the untrained eye cannot see the addition of this extra video field. In the figure, the film frames A-D are true or original images since they have been photographed as a complete frame. The A, B, and D frames on the right in the NTSC footage are original frames. The third and fourth frames have been created by blending movie fields from different frames.},",
				"   language = {en},",
				"   urldate = {2019-12-17},",
				"   journal = {Wikipedia},",
				"   month = may,",
				"   year = {2019},",
				"   note = {Page Version ID: 899459736}",
				"}"
			],
			"csl": {
				"URL": "https://en.wikipedia.org/w/index.php?title=Three-two_pull_down&oldid=899459736",
				"abstract": "Three-two pull down (3:2 pull down) is a term used in filmmaking and television production for the post-production process of transferring film to video. It converts 24 frames per second into 29.97 frames per second. Roughly speaking, converting every 4 frames into 5 frames plus a slight slow down in speed. Film runs at a standard rate of 24 frames per second, whereas NTSC video has a signal frame rate of 29.97 frames per second. Every interlaced video frame has two fields for each frame. The three-two pull down is where the telecine adds a third video field (a half frame) to every second video frame, but the untrained eye cannot see the addition of this extra video field. In the figure, the film frames A-D are true or original images since they have been photographed as a complete frame. The A, B, and D frames on the right in the NTSC footage are original frames. The third and fourth frames have been created by blending movie fields from different frames.",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"container-title": "Wikipedia",
				"id": "https://en.wikipedia.org/wiki/Three-two_pull_down",
				"issued": {
					"date-parts": [
						[
							2019,
							5
						]
					]
				},
				"note": "Page Version ID: 899459736",
				"title": "Three-two pull down",
				"type": "no-type"
			}
		},
		"https://github.com/mpv-player/mpv/wiki/Interpolation": {
			"fetched": "2019-12-17T23:18:56.595Z",
			"bibtex": [
				"",
				"@misc{noauthor__2019,",
				"   title = {mpv wiki interpolation},",
				"   url = {https://github.com/mpv-player/mpv},",
				"   urldate = {2019-12-17},",
				"   publisher = {mpv},",
				"   month = dec,",
				"   year = {2019},",
				"   note = {original-date: 2012-10-13T08:08:44Z}",
				"}"
			],
			"csl": {
				"URL": "https://github.com/mpv-player/mpv",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"id": "https://github.com/mpv-player/mpv/wiki/Interpolation",
				"issued": {
					"date-parts": [
						[
							2019,
							12
						]
					]
				},
				"note": "original-date: 2012-10-13T08:08:44Z",
				"publisher": "mpv",
				"title": "mpv wiki interpolation",
				"type": "no-type"
			}
		},
		"https://www.svp-team.com/": {
			"fetched": "2019-12-17T23:18:57.972Z",
			"bibtex": [
				"",
				"@misc{noauthor_svp_nodate,",
				"   title = {{SVP} – {SmoothVideo} {Project} – {Real} {Time} {Video} {Frame} {Rate} {Conversion}},",
				"   url = {https://www.svp-team.com/},",
				"   language = {en-US},",
				"   urldate = {2019-12-17}",
				"}"
			],
			"csl": {
				"URL": "https://www.svp-team.com/",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"id": "https://www.svp-team.com/",
				"title": "SVP – SmoothVideo Project – Real Time Video Frame Rate Conversion",
				"type": "no-type"
			}
		},
		"https://github.com/JesseMoll/CLFlow": {
			"fetched": "2019-12-17T23:20:17.431Z",
			"bibtex": [
				"",
				"@misc{moll_opencl_2018,",
				"   title = {{OpenCL} {Implementation} of the {Classic}+{NL} method for optical flow estimation: {JesseMoll}/{CLFlow}},",
				"   copyright = {MIT},",
				"   shorttitle = {{OpenCL} {Implementation} of the {Classic}+{NL} method for optical flow estimation},",
				"   url = {https://github.com/JesseMoll/CLFlow},",
				"   urldate = {2019-12-17},",
				"   author = {Moll, Jesse},",
				"   month = mar,",
				"   year = {2018},",
				"   note = {original-date: 2016-03-02T00:06:52Z}",
				"}"
			],
			"csl": {
				"URL": "https://github.com/JesseMoll/CLFlow",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"author": [
					{
						"family": "Moll",
						"given": "Jesse"
					}
				],
				"id": "https://github.com/JesseMoll/CLFlow",
				"issued": {
					"date-parts": [
						[
							2018,
							3
						]
					]
				},
				"note": "original-date: 2016-03-02T00:06:52Z",
				"title": "OpenCL Implementation of the Classic+NL method for optical flow estimation: JesseMoll/CLFlow",
				"title-short": "OpenCL Implementation of the Classic+NL method for optical flow estimation",
				"type": "no-type"
			}
		},
		"https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html": {
			"fetched": "2019-12-17T23:20:19.429Z",
			"bibtex": [
				"",
				"@misc{noauthor_opencv:_nodate,",
				"   title = {{OpenCV}: {Optical} {Flow}},",
				"   url = {https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html},",
				"   urldate = {2019-12-17},",
				"   journal = {docs.opencv.org}",
				"}"
			],
			"csl": {
				"URL": "https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"container-title": "docs.opencv.org",
				"id": "https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html",
				"title": "OpenCV: Optical Flow",
				"title-short": "OpenCV",
				"type": "no-type"
			}
		},
		"https://link.springer.com/chapter/10.1007/3-540-45103-X_50": {
			"fetched": "2019-12-17T23:20:20.620Z",
			"bibtex": [
				"",
				"@inproceedings{farneback_two-frame_2003,",
				"   series = {Lecture {Notes} in {Computer} {Science}},",
				"   title = {Two-{Frame} {Motion} {Estimation} {Based} on {Polynomial} {Expansion}},",
				"   isbn = {9783540451037},",
				"   url = {https://link.springer.com/chapter/10.1007/3-540-45103-X_50},",
				"   abstract = {This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.},",
				"   language = {en},",
				"   urldate = {2019-12-17},",
				"   booktitle = {Image {Analysis}},",
				"   publisher = {Springer Berlin Heidelberg},",
				"   author = {Farnebäck, Gunnar},",
				"   editor = {Bigun, Josef and Gustavsson, Tomas},",
				"   year = {2003},",
				"   keywords = {Computer Vision ,  Motion Model ,  Quadratic Polynomial ,  Polynomial Expansion ,  Orientation Tensor },",
				"   pages = {363--370}",
				"}"
			],
			"csl": {
				"ISBN": "9783540451037",
				"URL": "https://link.springer.com/chapter/10.1007/3-540-45103-X_50",
				"abstract": "This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"author": [
					{
						"family": "Farnebäck",
						"given": "Gunnar"
					}
				],
				"collection-title": "Lecture Notes in Computer Science",
				"container-title": "Image Analysis",
				"editor": [
					{
						"family": "Bigun",
						"given": "Josef"
					},
					{
						"family": "Gustavsson",
						"given": "Tomas"
					}
				],
				"id": "https://link.springer.com/chapter/10.1007/3-540-45103-X_50",
				"issued": {
					"date-parts": [
						[
							2003
						]
					]
				},
				"keyword": "Computer Vision , Motion Model , Quadratic Polynomial , Polynomial Expansion , Orientation Tensor ",
				"page": "363-370",
				"publisher": "Springer Berlin Heidelberg",
				"title": "Two-Frame Motion Estimation Based on Polynomial Expansion",
				"type": "paper-conference"
			}
		},
		"http://avisynth.nl/index.php/MVTools": {
			"fetched": "2019-12-17T23:20:22.557Z",
			"bibtex": [
				"",
				"@misc{noauthor_mvtools_nodate,",
				"   title = {{MVTools} - {Avisynth} wiki},",
				"   url = {http://avisynth.nl/index.php/MVTools},",
				"   urldate = {2019-12-17},",
				"   journal = {avisynth.nl}",
				"}"
			],
			"csl": {
				"URL": "http://avisynth.nl/index.php/MVTools",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"container-title": "avisynth.nl",
				"id": "http://avisynth.nl/index.php/MVTools",
				"title": "MVTools - Avisynth wiki",
				"type": "no-type"
			}
		},
		"https://www.khronos.org/assets/uploads/developers/library/2011_GDC_OpenCL/NVIDIA-OpenCL-Optical-Flow_GDC-Mar11.pdf": {
			"csl": {
				"id": "https://www.khronos.org/assets/uploads/developers/library/2011_GDC_OpenCL/NVIDIA-OpenCL-Optical-Flow_GDC-Mar11.pdf",
				"URL": "https://www.khronos.org/assets/uploads/developers/library/2011_GDC_OpenCL/NVIDIA-OpenCL-Optical-Flow_GDC-Mar11.pdf",
				"title": "OpenCL Imaging on The GPU: Optical Flow",
				"author": [
					{
						"family": "Fung",
						"given": "James"
					}
				],
				"issued": {
					"date-parts": [
						[
							2011,
							3
						]
					]
				}
			}
		},
		"https://www.intel.com/content/dam/www/programmable/us/en/pdfs/support/examples/download/exm_opencl_lucas_kanade_optical_flow_with_opencl.pdf": {
			"csl": {
				"id": "https://www.intel.com/content/dam/www/programmable/us/en/pdfs/support/examples/download/exm_opencl_lucas_kanade_optical_flow_with_opencl.pdf",
				"URL": "https://www.intel.com/content/dam/www/programmable/us/en/pdfs/support/examples/download/exm_opencl_lucas_kanade_optical_flow_with_opencl.pdf",
				"title": "Lucas KanadeOptical Flow –from C to OpenCL on CV SoC",
				"author": [
					{
						"family": "Denisenko",
						"given": "Dmitry"
					}
				],
				"issued": {
					"date-parts": [
						[
							2014,
							7,
							8
						]
					]
				}
			}
		},
		"https://arxiv.org/abs/1712.00080": {
			"fetched": "2019-12-17T23:36:03.308Z",
			"bibtex": [
				"",
				"@article{jiang_super_2017,",
				"   title = {Super {SloMo}: {High} {Quality} {Estimation} of {Multiple} {Intermediate} {Frames} for {Video} {Interpolation}},",
				"   shorttitle = {Super {SloMo}},",
				"   url = {http://arxiv.org/abs/1712.00080},",
				"   abstract = {Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.},",
				"   urldate = {2019-12-17},",
				"   journal = {arXiv:1712.00080 [cs]},",
				"   author = {Jiang, Huaizu and Sun, Deqing and Jampani, Varun and Yang, Ming-Hsuan and Learned-Miller, Erik and Kautz, Jan},",
				"   month = nov,",
				"   year = {2017},",
				"   note = {arXiv: 1712.00080},",
				"   keywords = {Computer Science - Computer Vision and Pattern Recognition}",
				"}"
			],
			"csl": {
				"URL": "http://arxiv.org/abs/1712.00080",
				"abstract": "Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"author": [
					{
						"family": "Jiang",
						"given": "Huaizu"
					},
					{
						"family": "Sun",
						"given": "Deqing"
					},
					{
						"family": "Jampani",
						"given": "Varun"
					},
					{
						"family": "Yang",
						"given": "Ming-Hsuan"
					},
					{
						"family": "Learned-Miller",
						"given": "Erik"
					},
					{
						"family": "Kautz",
						"given": "Jan"
					}
				],
				"container-title": "arXiv:1712.00080 [cs]",
				"id": "https://arxiv.org/abs/1712.00080",
				"issued": {
					"date-parts": [
						[
							2017,
							11
						]
					]
				},
				"keyword": "Computer Science - Computer Vision and Pattern Recognition",
				"note": "arXiv: 1712.00080",
				"title": "Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation",
				"title-short": "Super SloMo",
				"type": "article-journal"
			}
		},
		"https://cg.ivd.kit.edu/lehre/ws2020/index_2199.php": {
			"fetched": "2019-12-17T23:55:29.294Z",
			"bibtex": [
				"",
				"@misc{tessari_kit_2019,",
				"   type = {Text},",
				"   title = {{KIT} - {Computergrafik} - {WS} 2019/20},",
				"   copyright = {Alle Rechte liegen beim Autor siehe Impressum},",
				"   url = {https://cg.ivd.kit.edu/lehre/ws2020/index_2199.php},",
				"   language = {de-de},",
				"   urldate = {2019-12-17},",
				"   journal = {cg.ivd.kit.edu},",
				"   author = {Tessari, Lorenzo},",
				"   month = oct,",
				"   year = {2019}",
				"}"
			],
			"csl": {
				"URL": "https://cg.ivd.kit.edu/lehre/ws2020/index_2199.php",
				"accessed": {
					"date-parts": [
						[
							2019,
							12,
							17
						]
					]
				},
				"author": [
					{
						"family": "Tessari",
						"given": "Lorenzo"
					}
				],
				"container-title": "cg.ivd.kit.edu",
				"genre": "Text",
				"id": "https://cg.ivd.kit.edu/lehre/ws2020/index_2199.php",
				"issued": {
					"date-parts": [
						[
							2019,
							10
						]
					]
				},
				"title": "KIT - Computergrafik - WS 2019/20",
				"type": "no-type"
			}
		}
	}
}